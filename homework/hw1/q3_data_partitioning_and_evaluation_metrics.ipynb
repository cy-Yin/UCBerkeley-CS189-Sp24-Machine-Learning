{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q3a. Data partitioning\n",
    "\n",
    "mnist_data = np.load(\"data/mnist-data.npz\")\n",
    "spam_data = np.load(\"data/spam-data.npz\")\n",
    "\n",
    "fields = \"test_data\", \"training_data\", \"training_labels\"\n",
    "\n",
    "mnist_train_val_data = mnist_data[fields[1]]\n",
    "mnist_train_val_labels = mnist_data[fields[2]]\n",
    "spam_train_val_data = spam_data[fields[1]]\n",
    "spam_train_val_labels = spam_data[fields[2]]\n",
    "\n",
    "# For the MNIST dataset, write code that sets aside 10,000 training images as a validation set.\n",
    "mnist_val_data_num = 10000\n",
    "mnist_indices = np.random.permutation(len(mnist_train_val_data))\n",
    "\n",
    "mnist_val_data = mnist_train_val_data[mnist_indices[:mnist_val_data_num]]\n",
    "mnist_val_labels = mnist_train_val_labels[mnist_indices[:mnist_val_data_num]]\n",
    "mnist_train_data = mnist_train_val_data[mnist_indices[mnist_val_data_num:]]\n",
    "mnist_train_labels = mnist_train_val_labels[mnist_indices[mnist_val_data_num:]]\n",
    "\n",
    "# For the spam dataset, write code that sets aside 20% of the training data as a validation set.\n",
    "spam_val_data_split_ratio = 0.2\n",
    "spam_val_data_num = int(len(spam_train_val_data) * spam_val_data_split_ratio)\n",
    "spam_indices = np.random.permutation(len(spam_train_val_data))\n",
    "\n",
    "spam_val_data = spam_train_val_data[spam_indices[:spam_val_data_num]]\n",
    "spam_val_labels = spam_train_val_labels[spam_indices[:spam_val_data_num]]\n",
    "spam_train_data = spam_train_val_data[spam_indices[spam_val_data_num:]]\n",
    "spam_train_labels = spam_train_val_labels[spam_indices[spam_val_data_num:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q3b. Evaluation metric\n",
    "\n",
    "def eval_metric(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    use classification accuracy, or the percent of examples classified correctly,\n",
    "    as a measure of the classifier performance.\n",
    "    \n",
    "    Args:\n",
    "        true_labels: the set of true labels of the dataset\n",
    "        predicted_labels: the set of labels predicted by the model\n",
    "    \n",
    "    Returns:\n",
    "        the (unweighted) accuracy score\n",
    "    \"\"\"\n",
    "    assert len(true_labels) == len(predicted_labels)\n",
    "    total_num = len(true_labels)\n",
    "    accurate_num = 0\n",
    "    for i in range(total_num):\n",
    "        if true_labels[i] == predicted_labels[i]:\n",
    "            accurate_num += 1\n",
    "    \n",
    "    return accurate_num / total_num"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
